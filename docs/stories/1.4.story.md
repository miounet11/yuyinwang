# Story 1.4: 本地Whisper模型集成与优化

**Epic**: Epic 1 - Recording King 核心功能完善与体验优化  
**Story ID**: 1.4  
**Priority**: High  
**Estimate**: 8 Story Points  
**Status**: Draft  
**Assigned To**: TBD  
**Labels**: `local-whisper` `offline-transcription` `model-management` `gpu-acceleration`

---

## User Story

**As a** Recording King user,  
**I want** to use local Whisper models for offline transcription,  
**So that** I can continue using transcription functionality without network connectivity or when privacy protection is required.

---

## Acceptance Criteria

### AC1: Multiple Whisper Model Integration
- **Given** the application is installed
- **When** I access model management settings
- **Then** I can download and manage multiple Whisper model sizes (small/medium/large)
- **And** each model displays size, accuracy, and performance characteristics
- **And** model download progress is shown with pause/resume capability
- **And** model verification ensures integrity after download

### AC2: Model Download and Management System
- **Given** I want to use local transcription
- **When** I select a Whisper model to download
- **Then** the system downloads the model with progress indication
- **And** models are stored in a secure local directory
- **And** I can update, delete, or verify model files
- **And** storage usage is clearly displayed with cleanup options

### AC3: GPU Acceleration Support (Metal for macOS)
- **Given** I have a compatible Mac with Metal support
- **When** I enable GPU acceleration in settings
- **Then** Whisper models utilize Metal for faster processing
- **And** system detects GPU capabilities automatically
- **And** performance improvements are measurable and displayed
- **And** fallback to CPU occurs gracefully if GPU fails

### AC4: Seamless Local/Cloud Mode Switching
- **Given** I have both local models and cloud API configured
- **When** network connectivity changes or I manually switch modes
- **Then** transcription continues without interruption
- **And** the active mode is clearly indicated in the UI
- **And** transcription quality remains consistent across modes
- **And** user preferences for mode selection are remembered

### AC5: Performance Monitoring and Comparison
- **Given** I'm using local Whisper models
- **When** transcription is performed
- **Then** I can view performance metrics (speed, accuracy, resource usage)
- **And** compare local vs cloud performance side-by-side
- **And** receive recommendations for optimal model selection
- **And** performance data helps optimize system resource allocation

---

## Dev Notes

### Previous Story Insights
- **From Story 1.3**: Real-time transcription engine is now complete with streaming coordinator architecture
- **Integration Point**: Local Whisper models should integrate with existing `StreamingTranscriptionCoordinator` and `TranscriptionService`
- **Performance Requirements**: Must maintain <2s transcription latency established in Story 1.3

### Data Models
**Based on existing codebase analysis**:

#### LocalModelInfo Structure
[Source: src-tauri/src/transcription/whisper.rs + existing model cache implementation]
```rust
pub struct LocalModelInfo {
    pub model_id: String,
    pub model_name: String,           // "whisper-small", "whisper-medium", etc.
    pub file_path: PathBuf,
    pub size_bytes: u64,
    pub download_status: DownloadStatus,
    pub accuracy_score: Option<f64>,   // Estimated accuracy
    pub performance_rating: Option<f64>, // Processing speed rating
    pub supported_languages: Vec<String>,
    pub created_at: chrono::DateTime<chrono::Utc>,
    pub last_verified: Option<chrono::DateTime<chrono::Utc>>,
    pub metadata: serde_json::Value,   // Additional model metadata
}

pub enum DownloadStatus {
    NotDownloaded,
    Downloading { progress: f64 },
    Downloaded,
    Corrupted,
    UpdateAvailable,
}
```

#### ModelPerformanceMetrics Structure
[Source: src-tauri/src/performance_optimizer.rs integration]
```rust
pub struct ModelPerformanceMetrics {
    pub model_id: String,
    pub average_processing_time: Duration,
    pub gpu_acceleration_used: bool,
    pub memory_usage_mb: u32,
    pub cpu_usage_percent: f64,
    pub accuracy_samples: Vec<f64>,
    pub last_benchmark: chrono::DateTime<chrono::Utc>,
}
```

### API Specifications
**Integration with existing TranscriptionService**:

#### Local Model Management Commands
[Source: existing Tauri command patterns in src-tauri/src/commands/]

**List Available Models**:
- **Command**: `list_local_whisper_models`
- **Integration**: Extends existing transcription commands structure
```rust
#[tauri::command]
pub async fn list_local_whisper_models(
    state: State<'_, AppState>
) -> Result<Vec<LocalModelInfo>, String>
```

**Download Model**:
- **Command**: `download_whisper_model`
- **Integration**: Uses existing reqwest::Client in AppState
```rust
#[tauri::command]
pub async fn download_whisper_model(
    model_name: String,
    state: State<'_, AppState>
) -> Result<String, String>
```

**Switch Transcription Mode**:
- **Command**: `set_transcription_mode`
- **Integration**: Modifies existing TranscriptionConfig in AppState
```rust
#[tauri::command]
pub async fn set_transcription_mode(
    mode: TranscriptionMode, // Local | Cloud | Auto
    model_id: Option<String>,
    state: State<'_, AppState>
) -> Result<(), String>
```

### Component Specifications
**React Components Integration**:

#### ModelManagementPanel Component
[Source: existing UI patterns in src/components/]
- **Location**: `src/components/models/ModelManagementPanel.tsx`
- **Integration**: Uses existing Tauri invoke patterns and Zustand stores
- **Props**: 
```typescript
interface ModelManagementPanelProps {
  onModelChange: (modelId: string) => void;
  currentModel?: string;
  isVisible: boolean;
}
```

#### TranscriptionModeSelector Component
[Source: existing settings UI patterns]
- **Location**: `src/components/transcription/TranscriptionModeSelector.tsx`
- **State Management**: Integrates with existing transcription store
- **Features**: Local/Cloud/Auto mode selection with performance comparison

### File Locations
**Based on existing project structure**:

#### Backend Implementation
[Source: src-tauri/src/ directory structure]
- `src-tauri/src/transcription/local_whisper.rs` - Local Whisper service implementation
- `src-tauri/src/transcription/model_manager.rs` - Model download and management
- `src-tauri/src/commands/model_management.rs` - Tauri commands for model operations
- `src-tauri/src/models/local_models.rs` - Database models for local model info
- `src-tauri/src/performance/gpu_detector.rs` - Metal GPU detection and optimization

#### Frontend Implementation
[Source: src/ directory structure]
- `src/components/models/` - Model management UI components
- `src/stores/modelStore.ts` - Zustand store for model state management
- `src/types/models.ts` - TypeScript interfaces for model data
- `src/hooks/useLocalModels.ts` - React hooks for model operations

#### Database Schema Extensions
[Source: src-tauri/src/database/models.rs + migrations.rs]
```sql
-- Extends existing SQLite schema
CREATE TABLE local_whisper_models (
    id INTEGER PRIMARY KEY,
    model_id TEXT UNIQUE NOT NULL,
    model_name TEXT NOT NULL,
    file_path TEXT NOT NULL,
    size_bytes INTEGER NOT NULL,
    download_status TEXT NOT NULL,
    accuracy_score REAL,
    performance_rating REAL,
    supported_languages TEXT, -- JSON array
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    last_verified TIMESTAMP,
    metadata TEXT -- JSON
);

CREATE TABLE model_performance_metrics (
    id INTEGER PRIMARY KEY,
    model_id TEXT NOT NULL,
    average_processing_time_ms INTEGER NOT NULL,
    gpu_acceleration_used BOOLEAN NOT NULL,
    memory_usage_mb INTEGER NOT NULL,
    cpu_usage_percent REAL NOT NULL,
    accuracy_samples TEXT, -- JSON array
    last_benchmark TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    FOREIGN KEY (model_id) REFERENCES local_whisper_models(model_id)
);
```

### Testing Requirements
**Based on existing testing patterns**:

#### Unit Tests
[Source: existing test structure in src-tauri/src/]
- Model download and verification logic
- GPU detection and Metal acceleration
- Local/cloud mode switching
- Performance metrics collection

#### Integration Tests
[Source: existing integration testing patterns]
- End-to-end model download flow
- Transcription accuracy comparison (local vs cloud)
- Resource usage monitoring
- Error handling and fallback scenarios

### Technical Constraints
**From existing architecture and dependencies**:

#### Dependencies
[Source: Cargo.toml analysis]
- **whisper-rs**: Already integrated with Metal features enabled
- **reqwest**: Available for model downloads with progress tracking
- **tokio**: Async runtime available for background downloads
- **rusqlite + r2d2**: Database layer ready for model metadata storage

#### Performance Requirements
[Source: Story 1.3 established benchmarks]
- Local transcription latency: <2s (maintain parity with cloud)
- Model download: Progress updates every 100ms
- GPU detection: <500ms on app startup
- Mode switching: <1s transition time

#### Storage Requirements
- Model storage directory: `~/Library/Application Support/RecordingKing/models/`
- Disk space monitoring: Alert when <1GB free space available
- Model cleanup: Automatic removal of unused models after 30 days

#### Security Considerations
[Source: existing security patterns in project]
- Model file integrity verification using SHA256 checksums
- Secure download over HTTPS with certificate validation
- Local model file permissions: Read-only after download
- No sensitive data in model metadata storage

---

## Tasks / Subtasks

### Task 1: Model Management Infrastructure (AC: 1, 2)
1.1 Create `LocalModelInfo` and related data structures in `src-tauri/src/models/local_models.rs`
1.2 Implement database schema migration for local model tables
1.3 Create `ModelManager` service in `src-tauri/src/transcription/model_manager.rs`
1.4 Add model download with progress tracking using existing reqwest client
1.5 Implement model verification and integrity checking
1.6 Add unit tests for model management operations

### Task 2: Local Whisper Integration (AC: 1, 3)
2.1 Enhance existing `WhisperTranscriber` in `src-tauri/src/transcription/whisper.rs`
2.2 Implement Metal GPU detection and acceleration support
2.3 Integrate with existing performance optimizer for resource monitoring
2.4 Add model loading and caching mechanisms
2.5 Create fallback logic for GPU to CPU processing
2.6 Add integration tests for local transcription pipeline

### Task 3: Mode Switching and Configuration (AC: 4)
3.1 Extend `TranscriptionConfig` to support local/cloud mode selection
3.2 Implement seamless mode switching in `TranscriptionService`
3.3 Add automatic network detection and mode fallback
3.4 Integrate with existing `StreamingTranscriptionCoordinator`
3.5 Persist mode preferences in existing app settings
3.6 Add unit tests for mode switching logic

### Task 4: Performance Monitoring and Metrics (AC: 5)
4.1 Create `ModelPerformanceMetrics` tracking system
4.2 Implement benchmarking tools for local vs cloud comparison
4.3 Add resource usage monitoring (CPU, memory, GPU)
4.4 Create performance dashboard components
4.5 Implement optimization recommendations engine
4.6 Add performance tests and benchmarks

### Task 5: Frontend Model Management UI (AC: 1, 2, 5)
5.1 Create `ModelManagementPanel` component in `src/components/models/`
5.2 Implement model download progress UI with pause/resume
5.3 Add model comparison and selection interface
5.4 Create `TranscriptionModeSelector` component
5.5 Integrate with existing Zustand stores for state management
5.6 Add responsive design for model management settings

### Task 6: Tauri Commands and API Integration (AC: 1, 2, 3, 4)
6.1 Implement model management Tauri commands in `src-tauri/src/commands/model_management.rs`
6.2 Add GPU capability detection commands
6.3 Create transcription mode switching commands
6.4 Integrate commands with main application in `src-tauri/src/main.rs`
6.5 Add error handling and validation for all commands
6.6 Add integration tests for Tauri command layer

### Task 7: Documentation and Error Handling (AC: All)
7.1 Create user documentation for local model setup and usage
7.2 Implement comprehensive error handling for download failures
7.3 Add troubleshooting guides for GPU acceleration issues
7.4 Create migration guide from cloud-only to hybrid setup
7.5 Document performance optimization recommendations
7.6 Add logging and telemetry for model usage analytics

---

## Project Structure Notes

**Alignment with existing architecture**: The implementation follows the established modular architecture pattern with clear separation between:
- **Data Layer**: SQLite database extensions using existing migration patterns
- **Service Layer**: Integration with existing `TranscriptionService` and `PerformanceOptimizer`
- **Command Layer**: New Tauri commands following existing patterns in `src-tauri/src/commands/`
- **UI Layer**: React components using established patterns and Zustand state management

**No structural conflicts identified** - all new components integrate cleanly with existing codebase structure.

---

## Definition of Done

- [ ] All acceptance criteria verified through manual testing
- [ ] Unit test coverage >80% for new model management code
- [ ] Integration tests pass for all transcription modes
- [ ] Performance benchmarks meet <2s transcription latency requirement
- [ ] GPU acceleration functional on Metal-compatible Macs
- [ ] Model download/management UI complete and responsive
- [ ] Documentation updated with local model setup instructions
- [ ] Code review completed and approved
- [ ] No regressions in existing transcription functionality

---

**Created**: 2025-01-18  
**Last Updated**: 2025-01-18  
**Epic**: Epic 1 - Recording King 核心功能完善与体验优化