use std::path::Path;
use std::sync::Arc;
use std::time::{Duration, Instant};
use reqwest::Client;
use crate::errors::{AppError, AppResult};
use crate::types::{TranscriptionResult, TranscriptionConfig};
use super::{WhisperTranscriber, TranscriptionApiClient};
use parking_lot::Mutex;
use std::sync::atomic::{AtomicBool, Ordering};

pub struct TranscriptionService {
    whisper_transcriber: Arc<WhisperTranscriber>,
    api_client: Arc<TranscriptionApiClient>,
    openai_api_key: Option<String>,
    // æ™ºèƒ½æ¨¡å¼åˆ‡æ¢ç›¸å…³
    mode_selector: Arc<Mutex<IntelligentModeSelector>>,
}

impl TranscriptionService {
    pub fn new(http_client: Client, openai_api_key: Option<String>) -> Self {
        Self {
            whisper_transcriber: Arc::new(WhisperTranscriber::new()),
            api_client: Arc::new(TranscriptionApiClient::new(http_client)),
            openai_api_key,
            mode_selector: Arc::new(Mutex::new(IntelligentModeSelector::new())),
        }
    }

    /// è½¬å½•éŸ³é¢‘æ–‡ä»¶ï¼ˆæ™ºèƒ½æ¨¡å¼é€‰æ‹©ï¼‰
    pub async fn transcribe_audio<P: AsRef<Path>>(
        &self,
        audio_file_path: P,
        config: &TranscriptionConfig,
    ) -> AppResult<TranscriptionResult> {
        let path = audio_file_path.as_ref();
        
        // éªŒè¯éŸ³é¢‘æ–‡ä»¶å­˜åœ¨
        if !path.exists() {
            return Err(AppError::FileSystemError(format!("éŸ³é¢‘æ–‡ä»¶ä¸å­˜åœ¨: {:?}", path)));
        }

        // è®°å½•è½¬å½•å¼€å§‹
        println!("ğŸµ å¼€å§‹è½¬å½•éŸ³é¢‘æ–‡ä»¶: {:?}", path);
        println!("ğŸ”§ åŸå§‹é…ç½®: æ¨¡å‹={}, æœ¬åœ°={}, è¯­è¨€={:?}", 
                 config.model_name, config.is_local, config.language);
        
        // æ™ºèƒ½æ¨¡å¼é€‰æ‹©
        let optimized_config = {
            let selector = self.mode_selector.lock();
            drop(selector); // é‡Šæ”¾é”
            // æš‚æ—¶è·³è¿‡æ™ºèƒ½æ¨¡å¼é€‰æ‹©ï¼Œç›´æ¥ä½¿ç”¨åŸå§‹é…ç½®
            config.clone()
        };
        
        if optimized_config.is_local != config.is_local {
            println!("ğŸ§  æ™ºèƒ½æ¨¡å¼åˆ‡æ¢: {} -> {}", 
                    if config.is_local { "æœ¬åœ°" } else { "åœ¨çº¿" },
                    if optimized_config.is_local { "æœ¬åœ°" } else { "åœ¨çº¿" });
        }
        
        let start_time = Instant::now();
        let result = if optimized_config.is_local {
            // ä½¿ç”¨æœ¬åœ°Whisperè½¬å½•
            match optimized_config.model_name.as_str() {
                model_name if model_name.starts_with("whisper-") => {
                    println!("ğŸ” ä½¿ç”¨æœ¬åœ° Whisper æ¨¡å‹è¿›è¡Œè½¬å½•");
                    self.whisper_transcriber
                        .transcribe_with_local_whisper(path, &optimized_config)
                        .await
                },
                _ => {
                    Err(AppError::ValidationError(format!("ä¸æ”¯æŒçš„æœ¬åœ°æ¨¡å‹: {}", optimized_config.model_name)))
                }
            }
        } else {
            // ä½¿ç”¨APIè½¬å½•
            match optimized_config.model_name.as_str() {
                "luyin-api" | "luyingwang-online" => {
                    println!("ğŸ” ä½¿ç”¨å½•éŸ³ç‹APIè¿›è¡Œè½¬å½•");
                    self.api_client.transcribe_with_luyin_api(path).await
                },
                _ => {
                    println!("ğŸ” ä½¿ç”¨OpenAIå…¼å®¹APIè¿›è¡Œè½¬å½•");
                    let api_key = self.openai_api_key
                        .as_ref()
                        .ok_or_else(|| AppError::ConfigurationError("ç¼ºå°‘OpenAI APIå¯†é’¥".to_string()))?;
                    
                    self.api_client
                        .transcribe_with_openai_api(path, api_key, &optimized_config)
                        .await
                }
            }
        };

        let duration = start_time.elapsed();
        
        // è®°å½•æ€§èƒ½åé¦ˆ
        {
            let mut selector = self.mode_selector.lock();
            selector.record_performance(&optimized_config, duration, result.is_ok());
        }
        
        match &result {
            Ok(transcription) => {
                println!("âœ… è½¬å½•æˆåŠŸå®Œæˆï¼Œç»“æœé•¿åº¦: {} å­—ç¬¦ï¼Œè€—æ—¶: {:?}", 
                        transcription.text.len(), duration);
                if transcription.text.len() > 100 {
                    println!("ğŸ“ è½¬å½•å†…å®¹é¢„è§ˆ: {}...", &transcription.text[..100]);
                } else {
                    println!("ğŸ“ è½¬å½•å†…å®¹: {}", transcription.text);
                }
            },
            Err(e) => {
                println!("âŒ è½¬å½•å¤±è´¥: {}ï¼Œè€—æ—¶: {:?}", e, duration);
            }
        }

        result
    }

    /// è·å–å¯ç”¨çš„è½¬å½•æ¨¡å‹åˆ—è¡¨
    pub fn get_available_models(&self) -> Vec<String> {
        let mut models = vec![
            // æœ¬åœ°Whisperæ¨¡å‹
            "whisper-tiny".to_string(),
            "whisper-base".to_string(),
            "whisper-small".to_string(),
            "whisper-medium".to_string(),
            "whisper-large-v3".to_string(),
            "whisper-large-v3-turbo".to_string(),
            
            // APIæ¨¡å‹
            "whisper-1".to_string(),
            "gpt-4o-mini".to_string(),
            "luyin-api".to_string(),
        ];

        // æ ¹æ®APIå¯†é’¥å¯ç”¨æ€§è¿‡æ»¤æ¨¡å‹
        if self.openai_api_key.is_none() {
            models.retain(|model| {
                model.starts_with("whisper-") && !model.eq("whisper-1") || model.eq("luyin-api")
            });
        }

        models
    }

    /// æ£€æŸ¥æ¨¡å‹æ˜¯å¦å¯ç”¨
    pub async fn is_model_available(&self, model_name: &str) -> bool {
        match model_name {
            name if name.starts_with("whisper-") && name != "whisper-1" => {
                // æœ¬åœ°Whisperæ¨¡å‹ - æ£€æŸ¥æ˜¯å¦å¯ä»¥ä¸‹è½½
                true // å‡è®¾æ‰€æœ‰æ”¯æŒçš„æ¨¡å‹éƒ½å¯ç”¨
            },
            "luyin-api" => {
                // å½•éŸ³API - æ£€æŸ¥ç½‘ç»œè¿æ¥
                true // ç®€åŒ–å®ç°
            },
            "whisper-1" | "gpt-4o-mini" => {
                // OpenAI APIæ¨¡å‹ - æ£€æŸ¥APIå¯†é’¥
                self.openai_api_key.is_some()
            },
            _ => false
        }
    }

    /// åˆ›å»ºé»˜è®¤é…ç½®
    pub fn create_default_config(model_name: &str) -> TranscriptionConfig {
        let is_local = model_name.starts_with("whisper-") && model_name != "whisper-1";
        
        TranscriptionConfig {
            model_name: model_name.to_string(),
            language: Some("auto".to_string()),
            temperature: Some(0.0),
            is_local,
            api_endpoint: if is_local { 
                None 
            } else { 
                Some("https://api.openai.com/v1/audio/transcriptions".to_string()) 
            },
        }
    }

    /// éªŒè¯è½¬å½•é…ç½®
    pub fn validate_config(&self, config: &TranscriptionConfig) -> AppResult<()> {
        // éªŒè¯æ¨¡å‹åç§°
        let available_models = self.get_available_models();
        if !available_models.contains(&config.model_name) {
            return Err(AppError::ValidationError(format!("ä¸æ”¯æŒçš„æ¨¡å‹: {}", config.model_name)));
        }

        // éªŒè¯APIé…ç½®
        if !config.is_local {
            match config.model_name.as_str() {
                "luyin-api" => {
                    // å½•éŸ³APIä¸éœ€è¦é¢å¤–éªŒè¯
                },
                _ => {
                    if self.openai_api_key.is_none() {
                        return Err(AppError::ConfigurationError("OpenAI APIæ¨¡å‹éœ€è¦APIå¯†é’¥".to_string()));
                    }
                }
            }
        }

        // éªŒè¯æ¸©åº¦å‚æ•°
        if let Some(temp) = config.temperature {
            if !(0.0..=2.0).contains(&temp) {
                return Err(AppError::ValidationError("æ¸©åº¦å‚æ•°å¿…é¡»åœ¨0.0-2.0ä¹‹é—´".to_string()));
            }
        }

        Ok(())
    }

    /// æµ‹è¯•è½¬å½•æœåŠ¡
    pub async fn test_service(&self, config: &TranscriptionConfig) -> AppResult<bool> {
        // éªŒè¯é…ç½®
        self.validate_config(config)?;
        
        if config.is_local {
            // æµ‹è¯•æœ¬åœ°æ¨¡å‹å¯ç”¨æ€§
            println!("ğŸ§ª æµ‹è¯•æœ¬åœ°Whisperæ¨¡å‹: {}", config.model_name);
            // è¿™é‡Œå¯ä»¥æ·»åŠ æ¨¡å‹æ–‡ä»¶å­˜åœ¨æ€§æ£€æŸ¥
            Ok(true)
        } else {
            match config.model_name.as_str() {
                "luyin-api" => {
                    println!("ğŸ§ª æµ‹è¯•å½•éŸ³APIè¿æ¥");
                    // å¯ä»¥æ·»åŠ APIè¿é€šæ€§æµ‹è¯•
                    Ok(true)
                },
                _ => {
                    println!("ğŸ§ª æµ‹è¯•OpenAIå…¼å®¹APIè¿æ¥");
                    if let Some(api_key) = &self.openai_api_key {
                        let default_endpoint = "https://api.openai.com/v1/audio/transcriptions".to_string();
                        let api_endpoint = config.api_endpoint
                            .as_ref()
                            .unwrap_or(&default_endpoint);
                        self.api_client.test_api_connection(api_endpoint, api_key).await
                    } else {
                        Err(AppError::ConfigurationError("ç¼ºå°‘APIå¯†é’¥".to_string()))
                    }
                }
            }
        }
    }

    /// æ›´æ–°APIå¯†é’¥
    pub fn update_api_key(&mut self, api_key: Option<String>) {
        self.openai_api_key = api_key;
    }
    
    /// è·å–æ¨¡å¼é€‰æ‹©å™¨ç»Ÿè®¡ä¿¡æ¯
    pub fn get_mode_selector_stats(&self) -> ModeSelectionStats {
        self.mode_selector.lock().get_stats()
    }
    
    /// é‡ç½®æ¨¡å¼é€‰æ‹©å™¨ç»Ÿè®¡
    pub fn reset_mode_selector_stats(&self) {
        self.mode_selector.lock().reset_stats();
    }
}

/// æ™ºèƒ½æ¨¡å¼é€‰æ‹©å™¨
#[derive(Debug)]
struct IntelligentModeSelector {
    performance_history: std::collections::HashMap<String, Vec<PerformanceRecord>>,
    network_status: NetworkStatus,
    last_network_check: Option<Instant>,
    preferences: ModePreferences,
}

#[derive(Debug, Clone)]
struct PerformanceRecord {
    duration: Duration,
    success: bool,
    timestamp: Instant,
}

#[derive(Debug, Clone)]
enum NetworkStatus {
    Unknown,
    Online,
    Offline,
    Slow,
}

#[derive(Debug, Clone)]
struct ModePreferences {
    prefer_local: bool,
    max_acceptable_delay: Duration,
    quality_over_speed: bool,
}

#[derive(Debug, Clone, serde::Serialize, serde::Deserialize)]
pub struct ModeSelectionStats {
    pub total_selections: u32,
    pub local_selections: u32,
    pub online_selections: u32,
    pub auto_switches: u32,
    pub avg_local_duration_ms: u32,
    pub avg_online_duration_ms: u32,
    pub local_success_rate: f64,
    pub online_success_rate: f64,
}

impl IntelligentModeSelector {
    fn new() -> Self {
        Self {
            performance_history: std::collections::HashMap::new(),
            network_status: NetworkStatus::Unknown,
            last_network_check: None,
            preferences: ModePreferences {
                prefer_local: true,
                max_acceptable_delay: Duration::from_secs(30),
                quality_over_speed: false,
            },
        }
    }
    
    async fn select_optimal_mode(&mut self, config: &TranscriptionConfig) -> AppResult<TranscriptionConfig> {
        // å¦‚æœç”¨æˆ·æ˜ç¡®æŒ‡å®šæ¨¡å¼ï¼Œä¸è¿›è¡Œæ™ºèƒ½é€‰æ‹©
        if self.should_respect_user_choice(config) {
            return Ok(config.clone());
        }
        
        // æ›´æ–°ç½‘ç»œçŠ¶æ€
        self.update_network_status().await;
        
        let should_use_local = self.decide_mode(config).await;
        
        let mut optimized_config = config.clone();
        
        if should_use_local && !config.is_local {
            // åˆ‡æ¢åˆ°æœ¬åœ°æ¨¡å¼
            optimized_config.is_local = true;
            optimized_config.model_name = self.select_best_local_model();
        } else if !should_use_local && config.is_local {
            // åˆ‡æ¢åˆ°åœ¨çº¿æ¨¡å¼
            optimized_config.is_local = false;
            optimized_config.model_name = self.select_best_online_model();
        }
        
        Ok(optimized_config)
    }
    
    fn record_performance(&mut self, config: &TranscriptionConfig, duration: Duration, success: bool) {
        let key = format!("{}_{}", 
                         if config.is_local { "local" } else { "online" },
                         config.model_name);
        
        let record = PerformanceRecord {
            duration,
            success,
            timestamp: Instant::now(),
        };
        
        self.performance_history
            .entry(key)
            .or_insert_with(Vec::new)
            .push(record);
        
        // åªä¿ç•™æœ€è¿‘çš„50ä¸ªè®°å½•
        let records = self.performance_history.get_mut(&format!("{}_{}", 
                                                                if config.is_local { "local" } else { "online" },
                                                                config.model_name)).unwrap();
        if records.len() > 50 {
            records.drain(0..records.len() - 50);
        }
    }
    
    async fn update_network_status(&mut self) {
        let now = Instant::now();
        
        // æ¯30ç§’æ£€æŸ¥ä¸€æ¬¡ç½‘ç»œçŠ¶æ€
        if let Some(last_check) = self.last_network_check {
            if now.duration_since(last_check) < Duration::from_secs(30) {
                return;
            }
        }
        
        self.last_network_check = Some(now);
        
        // ç®€å•çš„ç½‘ç»œè¿é€šæ€§æ£€æŸ¥
        let client = reqwest::Client::builder()
            .timeout(Duration::from_secs(5))
            .build()
            .unwrap();
        
        match client.get("https://www.google.com").send().await {
            Ok(response) => {
                if response.status().is_success() {
                    self.network_status = NetworkStatus::Online;
                } else {
                    self.network_status = NetworkStatus::Slow;
                }
            }
            Err(_) => {
                self.network_status = NetworkStatus::Offline;
            }
        }
    }
    
    async fn decide_mode(&self, config: &TranscriptionConfig) -> bool {
        // å¦‚æœç½‘ç»œç¦»çº¿ï¼Œå¿…é¡»ä½¿ç”¨æœ¬åœ°æ¨¡å¼
        if matches!(self.network_status, NetworkStatus::Offline) {
            return true;
        }
        
        // å¦‚æœç”¨æˆ·åå¥½æœ¬åœ°å¤„ç†
        if self.preferences.prefer_local {
            return true;
        }
        
        // æ ¹æ®å†å²æ€§èƒ½å†³å®š
        let local_performance = self.get_average_performance("local", &config.model_name);
        let online_performance = self.get_average_performance("online", &config.model_name);
        
        match (local_performance, online_performance) {
            (Some(local), Some(online)) => {
                if self.preferences.quality_over_speed {
                    // ä¼˜å…ˆè€ƒè™‘æˆåŠŸç‡
                    local.success_rate >= online.success_rate
                } else {
                    // ä¼˜å…ˆè€ƒè™‘é€Ÿåº¦
                    local.avg_duration < online.avg_duration
                }
            }
            (Some(_), None) => true,   // åªæœ‰æœ¬åœ°å†å²è®°å½•
            (None, Some(_)) => false,  // åªæœ‰åœ¨çº¿å†å²è®°å½•
            (None, None) => self.preferences.prefer_local, // æ²¡æœ‰å†å²è®°å½•ï¼Œä½¿ç”¨åå¥½
        }
    }
    
    fn should_respect_user_choice(&self, config: &TranscriptionConfig) -> bool {
        // å¦‚æœç”¨æˆ·æ˜ç¡®é€‰æ‹©äº†ç‰¹å®šçš„APIæ¨¡å‹ï¼Œå°Šé‡ç”¨æˆ·é€‰æ‹©
        matches!(config.model_name.as_str(), "whisper-1" | "gpt-4o-mini" | "luyin-api")
    }
    
    fn select_best_local_model(&self) -> String {
        // æ ¹æ®æ€§èƒ½å†å²é€‰æ‹©æœ€ä½³æœ¬åœ°æ¨¡å‹
        let models = ["whisper-base", "whisper-small", "whisper-medium"];
        
        let best_model = models.iter()
            .min_by_key(|&model| {
                self.get_average_performance("local", model)
                    .map(|perf| perf.avg_duration.as_millis() as u64)
                    .unwrap_or(u64::MAX)
            })
            .unwrap_or(&"whisper-base");
        
        best_model.to_string()
    }
    
    fn select_best_online_model(&self) -> String {
        // é»˜è®¤ä½¿ç”¨å½•éŸ³API
        "luyin-api".to_string()
    }
    
    fn get_average_performance(&self, mode: &str, model: &str) -> Option<AveragePerformance> {
        let key = format!("{}_{}", mode, model);
        let records = self.performance_history.get(&key)?;
        
        if records.is_empty() {
            return None;
        }
        
        let total_duration: Duration = records.iter().map(|r| r.duration).sum();
        let success_count = records.iter().filter(|r| r.success).count();
        
        Some(AveragePerformance {
            avg_duration: total_duration / records.len() as u32,
            success_rate: success_count as f64 / records.len() as f64,
            sample_count: records.len(),
        })
    }
    
    fn get_stats(&self) -> ModeSelectionStats {
        let mut total_selections = 0u32;
        let mut local_selections = 0u32;
        let mut online_selections = 0u32;
        let mut local_durations = Vec::new();
        let mut online_durations = Vec::new();
        let mut local_successes = 0u32;
        let mut online_successes = 0u32;
        
        for (key, records) in &self.performance_history {
            total_selections += records.len() as u32;
            
            let is_local = key.starts_with("local_");
            
            for record in records {
                if is_local {
                    local_selections += 1;
                    local_durations.push(record.duration.as_millis() as u32);
                    if record.success {
                        local_successes += 1;
                    }
                } else {
                    online_selections += 1;
                    online_durations.push(record.duration.as_millis() as u32);
                    if record.success {
                        online_successes += 1;
                    }
                }
            }
        }
        
        let avg_local_duration = if local_durations.is_empty() {
            0
        } else {
            local_durations.iter().sum::<u32>() / local_durations.len() as u32
        };
        
        let avg_online_duration = if online_durations.is_empty() {
            0
        } else {
            online_durations.iter().sum::<u32>() / online_durations.len() as u32
        };
        
        ModeSelectionStats {
            total_selections,
            local_selections,
            online_selections,
            auto_switches: 0, // æš‚æ—¶è®¾ä¸º0ï¼Œéœ€è¦é¢å¤–è·Ÿè¸ª
            avg_local_duration_ms: avg_local_duration,
            avg_online_duration_ms: avg_online_duration,
            local_success_rate: if local_selections > 0 {
                local_successes as f64 / local_selections as f64
            } else {
                0.0
            },
            online_success_rate: if online_selections > 0 {
                online_successes as f64 / online_selections as f64
            } else {
                0.0
            },
        }
    }
    
    fn reset_stats(&mut self) {
        self.performance_history.clear();
    }
}

#[derive(Debug, Clone)]
struct AveragePerformance {
    avg_duration: Duration,
    success_rate: f64,
    sample_count: usize,
}