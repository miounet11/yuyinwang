/**
 * Recording King æ€§èƒ½ä¼˜åŒ–æ¨¡å—
 * ä¸“é—¨ç”¨äºGPUä½¿ç”¨ä¼˜åŒ–å’Œè½¬å½•å»¶è¿Ÿå‡å°‘
 */

use whisper_rs::{WhisperContext, WhisperContextParameters, FullParams, SamplingStrategy};
use std::collections::HashMap;
use std::sync::{Arc, Mutex};
use once_cell::sync::Lazy;
use sysinfo::System;

// å…¨å±€æ¨¡å‹ç¼“å­˜ï¼Œé¿å…é‡å¤åŠ è½½
static MODEL_CACHE: Lazy<Arc<Mutex<HashMap<String, Arc<WhisperContext>>>>> = 
    Lazy::new(|| Arc::new(Mutex::new(HashMap::new())));

// æ€§èƒ½ç›‘æ§ç»“æ„
#[derive(Debug, Clone, serde::Serialize, serde::Deserialize)]
pub struct PerformanceMetrics {
    pub model_load_time_ms: u64,
    pub audio_processing_time_ms: u64,
    pub transcription_time_ms: u64,
    pub total_time_ms: u64,
    pub gpu_memory_usage_mb: f64,
    pub cpu_usage_percent: f64,
    pub audio_duration_seconds: f64,
    pub real_time_factor: f64, // è½¬å½•æ—¶é—´ / éŸ³é¢‘æ—¶é•¿
}

impl Default for PerformanceMetrics {
    fn default() -> Self {
        Self {
            model_load_time_ms: 0,
            audio_processing_time_ms: 0,
            transcription_time_ms: 0,
            total_time_ms: 0,
            gpu_memory_usage_mb: 0.0,
            cpu_usage_percent: 0.0,
            audio_duration_seconds: 0.0,
            real_time_factor: 0.0,
        }
    }
}

#[derive(Debug)]
pub struct PerformanceOptimizer {
    system: System,
    enable_gpu: bool,
    enable_caching: bool,
    max_cache_size: usize,
}

impl PerformanceOptimizer {
    pub fn new() -> Self {
        Self {
            system: System::new_all(),
            enable_gpu: true,
            enable_caching: true,
            max_cache_size: 3, // æœ€å¤šç¼“å­˜3ä¸ªæ¨¡å‹
        }
    }
    
    pub fn configure(&mut self, enable_gpu: bool, enable_caching: bool, max_cache_size: usize) {
        self.enable_gpu = enable_gpu;
        self.enable_caching = enable_caching;
        self.max_cache_size = max_cache_size;
    }
    
    // è·å–æˆ–åŠ è½½æ¨¡å‹ï¼ˆå¸¦ç¼“å­˜ï¼‰
    pub fn get_cached_model(&self, model_path: &str) -> Result<Arc<WhisperContext>, String> {
        if !self.enable_caching {
            return self.load_model_direct(model_path);
        }
        
        let start_time = std::time::Instant::now();
        
        // æ£€æŸ¥ç¼“å­˜
        {
            let cache = MODEL_CACHE.lock().unwrap();
            if let Some(cached_model) = cache.get(model_path) {
                println!("ğŸ¯ ä½¿ç”¨ç¼“å­˜çš„æ¨¡å‹: {}", model_path);
                return Ok(Arc::clone(cached_model));
            }
        }
        
        // ç¼“å­˜æœªå‘½ä¸­ï¼ŒåŠ è½½æ–°æ¨¡å‹
        println!("ğŸ“¦ æ¨¡å‹æœªç¼“å­˜ï¼Œæ­£åœ¨åŠ è½½: {}", model_path);
        let model = self.load_model_direct(model_path)?;
        
        // æ·»åŠ åˆ°ç¼“å­˜
        {
            let mut cache = MODEL_CACHE.lock().unwrap();
            
            // å¦‚æœç¼“å­˜å·²æ»¡ï¼Œç§»é™¤æœ€æ—§çš„æ¨¡å‹
            if cache.len() >= self.max_cache_size {
                if let Some(oldest_key) = cache.keys().next().cloned() {
                    cache.remove(&oldest_key);
                    println!("ğŸ—‘ï¸ ç§»é™¤æ—§æ¨¡å‹ç¼“å­˜: {}", oldest_key);
                }
            }
            
            cache.insert(model_path.to_string(), Arc::clone(&model));
        }
        
        let load_time = start_time.elapsed().as_millis();
        println!("âš¡ æ¨¡å‹åŠ è½½æ—¶é—´: {}ms", load_time);
        
        Ok(model)
    }
    
    // ç›´æ¥åŠ è½½æ¨¡å‹ï¼ˆGPUä¼˜åŒ–ï¼‰
    fn load_model_direct(&self, model_path: &str) -> Result<Arc<WhisperContext>, String> {
        let mut params = WhisperContextParameters::default();
        
        if self.enable_gpu {
            // å¯ç”¨GPUä¼˜åŒ–å‚æ•°
            params.use_gpu = true;
            
            // Metalç‰¹å®šä¼˜åŒ–è®¾ç½®
            #[cfg(target_os = "macos")]
            {
                // macOSä¸Šä¼˜åŒ–Metalä½¿ç”¨
                params.gpu_device = 0; // ä½¿ç”¨é»˜è®¤GPUè®¾å¤‡
                println!("ğŸš€ å¯ç”¨GPUåŠ é€Ÿï¼ˆMetal on macOSï¼‰");
                
                // æ£€æŸ¥Metalå¯ç”¨æ€§
                if self.check_metal_availability() {
                    println!("âœ… Metal GPUåŠ é€Ÿå¯ç”¨");
                } else {
                    println!("âš ï¸ Metal GPUåŠ é€Ÿä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨CPU");
                    params.use_gpu = false;
                }
            }
            
            #[cfg(not(target_os = "macos"))]
            {
                println!("ğŸš€ å¯ç”¨GPUåŠ é€Ÿ");
            }
        }
        
        let ctx = WhisperContext::new_with_params(model_path, params)
            .map_err(|e| format!("æ¨¡å‹åŠ è½½å¤±è´¥: {}", e))?;
            
        Ok(Arc::new(ctx))
    }
    
    /// æ£€æŸ¥Metalå¯ç”¨æ€§ï¼ˆmacOSä¸“ç”¨ï¼‰
    #[cfg(target_os = "macos")]
    fn check_metal_availability(&self) -> bool {
        // è¿™é‡Œå¯ä»¥æ·»åŠ Metalè®¾å¤‡æ£€æŸ¥é€»è¾‘
        // ç›®å‰ç®€å•è¿”å›trueï¼Œå‡è®¾Metalå¯ç”¨
        true
    }
    
    #[cfg(not(target_os = "macos"))]
    fn check_metal_availability(&self) -> bool {
        false
    }
    
    // ä¼˜åŒ–çš„è½¬å½•å‚æ•°
    pub fn get_optimized_transcription_params(&self) -> FullParams {
        let mut params = FullParams::new(SamplingStrategy::Greedy { best_of: 1 });
        
        // ä¼˜åŒ–å‚æ•°é…ç½®
        params.set_language(Some("auto"));
        params.set_translate(false);
        params.set_print_special(false);
        params.set_print_progress(false);
        params.set_print_realtime(false);
        params.set_print_timestamps(false);
        
        // æ€§èƒ½ä¼˜åŒ–è®¾ç½®
        params.set_n_threads(num_cpus::get() as i32 / 2); // ä½¿ç”¨ä¸€åŠCPUæ ¸å¿ƒ
        params.set_no_context(false); // å¯ç”¨ä¸Šä¸‹æ–‡ä»¥æé«˜å‡†ç¡®æ€§
        params.set_single_segment(false); // å…è®¸åˆ†æ®µå¤„ç†é•¿éŸ³é¢‘
        
        // GPUä¼˜åŒ–
        if self.enable_gpu {
            // Metalç‰¹å®šä¼˜åŒ–å°†ç”±whisper-rsåº“è‡ªåŠ¨å¤„ç†
            println!("âš¡ è½¬å½•å‚æ•°å·²é’ˆå¯¹GPUä¼˜åŒ–");
        }
        
        params
    }
    
    // å¿«é€ŸéŸ³é¢‘é¢„å¤„ç†
    pub fn preprocess_audio_fast(&self, audio_data: &[f32], sample_rate: u32) -> Result<Vec<f32>, String> {
        const TARGET_SAMPLE_RATE: u32 = 16000;
        
        if sample_rate == TARGET_SAMPLE_RATE {
            return Ok(audio_data.to_vec());
        }
        
        let start_time = std::time::Instant::now();
        
        // é«˜æ•ˆé‡é‡‡æ ·ç®—æ³•
        let ratio = TARGET_SAMPLE_RATE as f32 / sample_rate as f32;
        let new_length = (audio_data.len() as f32 * ratio) as usize;
        
        // ä½¿ç”¨çº¿æ€§æ’å€¼è¿›è¡Œå¿«é€Ÿé‡é‡‡æ ·
        let mut resampled = Vec::with_capacity(new_length);
        
        for i in 0..new_length {
            let src_index = (i as f32 / ratio).floor() as usize;
            let next_index = (src_index + 1).min(audio_data.len() - 1);
            let frac = (i as f32 / ratio) - src_index as f32;
            
            if src_index < audio_data.len() {
                // çº¿æ€§æ’å€¼
                let current = audio_data[src_index];
                let next = audio_data[next_index];
                let interpolated = current + frac * (next - current);
                resampled.push(interpolated);
            }
        }
        
        let processing_time = start_time.elapsed().as_millis();
        println!("ğŸµ éŸ³é¢‘é‡é‡‡æ ·æ—¶é—´: {}ms ({}Hz -> {}Hz)", 
                processing_time, sample_rate, TARGET_SAMPLE_RATE);
        
        Ok(resampled)
    }
    
    // è·å–ç³»ç»Ÿæ€§èƒ½æŒ‡æ ‡
    pub fn get_system_metrics(&mut self) -> Result<(f64, f64), String> {
        self.system.refresh_all();
        
        let cpu_usage = self.system.global_cpu_info().cpu_usage() as f64;
        let _total_memory = self.system.total_memory() as f64 / 1024.0 / 1024.0; // MB
        let used_memory = self.system.used_memory() as f64 / 1024.0 / 1024.0; // MB
        
        Ok((cpu_usage, used_memory))
    }
    
    // è®¡ç®—å®æ—¶å› å­ï¼ˆReal-Time Factorï¼‰
    pub fn calculate_rtf(&self, transcription_time_ms: u64, audio_duration_seconds: f64) -> f64 {
        let transcription_time_seconds = transcription_time_ms as f64 / 1000.0;
        if audio_duration_seconds > 0.0 {
            transcription_time_seconds / audio_duration_seconds
        } else {
            0.0
        }
    }
    
    // æ¸…ç†æ¨¡å‹ç¼“å­˜
    pub fn clear_model_cache(&self) {
        let mut cache = MODEL_CACHE.lock().unwrap();
        cache.clear();
        println!("ğŸ§¹ å·²æ¸…ç†æ‰€æœ‰æ¨¡å‹ç¼“å­˜");
    }
    
    // è·å–ç¼“å­˜ç»Ÿè®¡
    pub fn get_cache_stats(&self) -> (usize, Vec<String>) {
        let cache = MODEL_CACHE.lock().unwrap();
        let count = cache.len();
        let models: Vec<String> = cache.keys().cloned().collect();
        (count, models)
    }
    
    // é¢„çƒ­GPUï¼ˆå¦‚æœæ”¯æŒï¼‰
    pub fn warmup_gpu(&self) -> Result<(), String> {
        if !self.enable_gpu {
            return Ok(());
        }
        
        println!("ğŸ”¥ GPUé¢„çƒ­ä¸­...");
        
        // è¿™é‡Œå¯ä»¥æ·»åŠ GPUé¢„çƒ­é€»è¾‘
        // å¯¹äºMetalï¼Œwhisper-rsä¼šè‡ªåŠ¨å¤„ç†
        
        println!("âœ… GPUé¢„çƒ­å®Œæˆ");
        Ok(())
    }
}

// æ€§èƒ½ä¼˜åŒ–é…ç½®
#[derive(Debug, Clone, serde::Serialize, serde::Deserialize)]
pub struct PerformanceConfig {
    pub enable_gpu: bool,
    pub enable_model_caching: bool,
    pub max_cache_size: usize,
    pub target_rtf: f64, // ç›®æ ‡å®æ—¶å› å­ï¼ˆ< 1.0 è¡¨ç¤ºæ¯”å®æ—¶æ›´å¿«ï¼‰
    pub cpu_threads: Option<i32>,
    pub enable_audio_preprocessing: bool,
}

impl Default for PerformanceConfig {
    fn default() -> Self {
        Self {
            enable_gpu: true,
            enable_model_caching: true,
            max_cache_size: 3,
            target_rtf: 0.3, // ç›®æ ‡ï¼šè½¬å½•é€Ÿåº¦æ¯”å®æ—¶å¿«3å€
            cpu_threads: None, // è‡ªåŠ¨æ£€æµ‹
            enable_audio_preprocessing: true,
        }
    }
}

// å¯¼å‡ºå‡½æ•°ä¾›å¤–éƒ¨ä½¿ç”¨
pub fn create_performance_optimizer() -> PerformanceOptimizer {
    PerformanceOptimizer::new()
}

pub fn get_optimal_cpu_threads() -> i32 {
    let total_cores = num_cpus::get() as i32;
    // ä½¿ç”¨æ€»æ ¸å¿ƒæ•°çš„60-80%ï¼Œä¸ºç³»ç»Ÿä¿ç•™ä¸€äº›èµ„æº
    std::cmp::max(1, (total_cores as f32 * 0.7) as i32)
}